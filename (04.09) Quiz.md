## 동건

### 문제 1
epoch와 batch_size의 의미는 무엇이고 batch를 여러 개로 나누는 이유는?

---
### 풀이
- 동건) epoch: 전체 데이터를 활용한 학습 횟수.(Step 횟수와 같은 의미 -> 지금까지 range()로 표현되었던 부분)  
batch_size: 한 번의 epoch을 진행할 때, 모든 데이터를 메모리에 올려놓지 않고 나누어 진행하기 위해 만든 batch의 크기.  
대용량 데이터를 사용할 때, 메모리를 효율적으로 사용하기 위해서  
- 정우)  전체 데이터셋을 한번 학습한것(돈것),한번에 학습시킬 데이터의 크기, 데이터가 너무크기때문에 나눠서 진행함
- 현아) epoch: 전체 dataset을 한 번 다 학습시키는 것 / batch_size: 한 번에 학습시키는 데이터의 크기 / batch를 여러 개로 나누는 이유: data가 많을 경우 한 번에 메모리에 넣어 학습시킬 수 없기 때문에 
- 아영)  하나의 epoch - 전체 데이터 셋을 다 학습시킨 것 / batch_size - 전체 사이즈를 몇 개씩 나누는지
전체 사이즈가 너무 크기 때문에
- 승렬) batch_size란 한번에 몇개씩을 학습시킬까 하는 것이고 epoch는 전체 데이터셋을 학습시키는 횟수를 뜻한다. batch를 나누는 이유는 데이터가 너무 클 때 메모리 부족 문제로 한번에 다 돌릴 수 없기 때문이다.
- 지원) epoch는 알고리즘이 전체 훈련 데이터셋을 반복해서 학습하는 횟수를 의미한다. batch_size는 큰 training set을 몇 개로 나누었냐는 것이고, learning accuracy를 늘리기 위해 여러개로 나눈다.

### 문제 2
tf.reduce_mean([1, 2], axis = 0).eval() 의 값이 1인 이유는?

---
### 풀이
- 동건) input의 자료형이 int이기 때문에 output의 자료형도 int, 실수값을 반환하기 위해서는 뒤에. 을 붙여야한다.  
- 정우) float 형식으로 넣어야하는데 integer로 값을 넣었기 때문에   
- 현아) 1, 2의 자료형이 integer(정수형)이므로.
- 아영) 1과 2가 integer로 설정되어 있으므로 값도 integer로 나옴
- 승렬) 값이 integer로 나오기 때문에 소수점이 없다.
- 지원)  1과 2의 평균을 구하면 1.5가 나와야하지만 int값을 입력했기 때문에 결과값도 int값으로 출력된다. Float형을 int값으로 변환하면 소수점 밑의 숫자가 원래 버려졌던가 반올림이 아니었나.. 헷갈린다 ㅎㅅㅎ 근데 소수점 밑의 숫자가 버려지는게 맡는 것 같다 :) 

## 정우

### 문제1
learning late를 제대로 설정했다고 가정했을때 nan이 나오는 이유는 무엇이며, 이를 해결하기 위한 방법과 그 방법에 가장 많이 사용되는 코드를 써보시오(어떤 코드가 쓰이는지만 적으면 됩니다.)

---

### 풀이
- 동건)  학습률이 너무 커서 발산이 일어나면, nan값이 나온다. learning late를 낮춘다. 
- 정우)  오타났네요... input 데이터가 normalize 되지 않아서 그렇습니다. MinMaxscaler가 사용됩니다.
- 현아) learning rate이 적절해도 NaN이 나오는 이유는 너무 큰 값의 데이터가 있거나, 데이터의 크기가 들쭉날쭉 할 때, 즉 non-normalized data 때문이다. 이를 해결하는 방법은 데이터를 normalize하는 것이고, 코드로 표현하면 xy = MinMaxScaler(xy)
- 아영)  data가 non-normalized하기 때문에 / X_std[:, 0] = (X[:, -] - X[:,0].mean()) / X[:,0].std() 
- 승렬) 학습 데이터의 어레이 안의 값들이 너무 큰 차이가 날 때 nan이 나올 수 있다. 해결은 normalization으로 할 수 있으며 코드는
       xy = MinMaxScaler(xy) 이다.
- 지원) Data preprocessing(normalized)을 했는지 점검하고 standadiztion하기 / X_std[:0,]=(X[:,0]-X[:,0].mean())/X[:,0].std()

### 문제2

t = np.array([[[0,2], [0,1]],[[0,3], [3,1]], [[3,1],[4,3]]])일 때 tf.reshape을 이용하여 

       array([[[0, 2],
        [0, 1]],

       [[0, 3],
        [3, 1]],

       [[3, 1],
        [4, 3]]]) 이런 모양으로 바꾸기 위한 코드를 작성하시오
---

### 풀이 
- 동건)  tf.reshape(t, shape = [3,2,2]).eval()
- 정우)  tf.reshape(t, shape = [-1, 2, 2]).eval()
- 현아) ????? 원래 t의 shape은 (3,2,2)이고, 바꾸고자 하는 shape도 (3,2,2)이다.
- 아영)  tf.reshape(t, shape=[-1, 2, 2]).eval()
- 승렬) 그대로인걸...? tf.reshpae(t, shape=[-1,2,2])
- 지원) tf.reshape(t, shape=[3(or -1), 2, 2]).eval()   

## 현아

### 문제1
Regularization이 어떤 경우에 사용되는지 서술하고, regularization을 tensorflow 코드로 작성하시오.

---

### 풀이
=======
- 동건) overfitting을 막기위해 사용. 가중치 부여 방식에 따라 L1,L2 규제 등이 있다. 
- 정우)  weight이 너무 커서 overfitting 될 때 weight을 줄이기 위해서 사용됨, l2reg = 0.001 * tf.reduce_sum(tf.square(W))
- 현아)  regularization은 overfitting 문제를 해결하기 위해 사용됩니다. 코드는 l2reg = 0.001 * tf.reduce_sum(tf.square(W))를 선언한 후, 기존의 cost function에 l2reg를 더하면 됩니다.
- 아영)  learning rate를 잘 설정했는데 cost function이 제대로 동작하지 않는 경우 /l2reg=0.001*tf.reduce_sum(tf.square(W)) 
- 승렬) overfitting을 줄이기 위해 사용한다. l2reg = gamma*tf.reduce_sum(tf.square(W)) 을 cost function뒤에 더한 후 초기화.
- 지원)  Solutions for overfitting /  l2reg=0.001*tf.reduce_sum(tf.square(W))   

### 문제 2
matrix1 = tf.constant([[3., 2.], [1., 5.]])
matrix2 = tf.constant([7.], [4.])
일 때, 

tf.matmul(matrix1, matrix2).eval()의 결과값과
(matrix * matrix2).eval()의 결과값을 각각 구하시오.

---
### 풀이
=======
- 동건)  array([[29.],[27.]]), array([[21., 14.],[ 4., 20.]])
- 정우) array([[29.], [27.]]) / array([[21. , 14.] , [ 4., 20.]]) 
- 현아) array([29.], [27.]], dtype=float32) /// array([[21., 14.], [4., 20.]], dtype=float32) 
- 아영) ) [[29.],[11.]]
[[21., 14.], [4., 20.]] 
- 승렬)  
array([[29.],
       [27.]], dtype=float32)
array([[21., 14.]
       [ 4., 20.]], dtype=float32)
- 지원) array([[29.],[27.], dtype=float32] / array([[21., 14.],[4., 20.]], dtype=float32) 

## 아영

### 문제1
learning rate를 잘 설정했는데도 cost function이 제대로 동작하지 않는 경우 취할 수 있는 해결책은?

---
### 풀이
- 동건) 학습횟수를 늘린다.  
- 정우) input 데이터가 normalize되지 않았을 수 있기 때문에 데이터를 normalize 해줘야한다. minmaxscaler
- 현아) data의 값이 너무 크게 차이가 나는지 확인하고, 크게 차이가 난다면 data preprocessing(특히 standardization의 방법)을 활용한다. 
- 아영)  데이터 정규화
- 승렬) dataset을 normalize한다.
- 지원) normalized, standardization   

### 문제2
x = [1, 4]
y = [2, 5]
z = [3, 6] 일때
tf.stack([x, y, z], axis=0).eval()의 결과는?

---
### 풀이
- 동건) array([[1, 4],[2, 5],[3, 6]])  
- 정우) array([[1, 4], [2, 5], [3, 6]]) 
- 현아) 
array([[1, 4],
       [2, 5],
       [3, 6]])
- 아영)  [[1, 4], [2, 5], [3, 6]] 
- 승렬)  
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)
- 지원)  axis 개념을 이해해야 풀 수 있는 것 같은데 axis가 이해되지 않아서요.. Christine의 친절한 설명 기대할게요 ^^..

## 승렬

### 문제 1.
어떠한 경우에 data가 preprocessing이 필요한지 설명하고 그 방법들에 대해 설명하시오.

---

### 풀이
- 동건) 변수들의 측정단위가 달라서 range의 차이가 크거나, 집적기준 등이 달라서 공정한 비교가 필요할 때 사용한다. 
모든 변수들을 표준화하거나, 0~1사이의 범위로 바꾸거나, 정규화하거나 다양한 방법들이 있다.      
- 정우)  값들의 편차가 매우커서 그래프(등고선)을 그렸을 때 곡된 경우, learning rate 값이 좋아도 값이 튀는 경우가 발생한다. zero-center(데이터의 중심을 0으로 맞추는 방법, normalize(값 전체의 범위가 어떤 범위안에 항상들어가도록 만드는 방법 ex:standardization) 등의 방법이있다.
- 현아) data들의 값이 너무 큰 차이가 나는 경우 data preprocessing이 필요하다. 이렇게 data를 normalize하는 방법들로는, zero-centered data로 만드는 것(데이터의 분포의 중심이 0이 되도록 맞춤)과 standardization을 하는 것(데이터에서 평균을 뺀 값을 표준편차로 나눔)이 있다.
- 아영)  데이터 값에 큰 차이가 있어서 경사하강법을 적용하기 힘든 경우 / zero-centered data (data 중심이 0으로 갈수록), normalized data(데이터 전체의 범위가 어떤 값 안에 들어갈 수 있도록)
- 승렬) x 행렬의 열들 사이에 값의 차이가 너무 클때 preprocessing이 필요하고, 방법은 zero-center로 만들기, normalizaion이 있다.
- 지원) 데이터 간의 차이가 너무 크면 normalize 할 필요가 있음, 데이터의 중심이 0으로 세팅된 zero-centered data와 데이터 값이 특정 범위 내에 있도록 세팅된 normalized data가 있고, standardization이 가장 흔한 전처리 방법임.   

### 문제 2.
다음 코드의 결과값을 구하시오.
x=[[[[1.,2.,3.,4.],[5.,6.,7.,8.],[9.,10.,11.,12.]],[[13.,14.,15.,16.],[17.,18.,19.,20.],[21.,22.,23.,24.]]]]

tf.reduce_mean(x, axis=2).eval()

---

### 풀이 
- 동건)  array([[[ 5.,  6.,  7.,  8.],[17., 18., 19., 20.]]])
- 정우) array([[[ 5.,  6.,  7.,  8.], [17., 18., 19., 20.]]], dtype=float32)  
- 현아) [[[5,6,7,8],[17,18,19,20]]]    (참고: x의 shape은 (1,2,3,4)임)
- 아영)  [[[5. 6. 7. 8.],[17. 18. 19. 20.]]] 
- 승렬) array([[[5., 6., 7., 8.], [17., 18., 19., 20.]]], dtype=float32)
- 지원) axis 개념을 이해해야 풀 수 있는 것 같은데 axis가 이해되지 않아서요.. Sunny의 친절한 설명 기대할게요 ^^..    

## 지원

### 문제 1
Learning rate을 0.5로 설정하고 머신러닝을 돌렸더니 cost가 발산해버렸다. 이 문제를 영어 단어로 표현하고, 문제 해결 방법을 서술하시오.

---

### 풀이 
- 동건) 학습률을 0.5이하로 낮춘다.   
- 정우) overshooting, learning rate를 줄여준다  
- 현아) overshooting의 문제가 발생한 것이다. 이를 해결하려면 Learning rate를 적정 수준으로 내려야 한다. (참고: 일반적으로 많이 쓰는 learning rate = 0.1)  
- 아영) overshooting, learning rate를 작게 설정한다
- 승렬) overshooting. learning rate를 줄이면 된다.
- 지원) Overshooting, learning rate를 작게 조정한다.

---

### 문제 2
다음과 같이 shape이 다른 matrix를 계산할 수 있도록 작업하는 것을 지칭하는 (영어) 단어와 결과값을 작성하시오.

```

Matrix1=tf.constant([[1.,2.]]) 
Matrix2=tf.constant([[1.],[2.],[3.]])
(Matrix1+Matrix2).eval()

```

---

### 풀이
- 동건) broadcasting, array([[2., 3.],[3., 4.],[4., 5.]])
- 정우) broadcasting, array([[2., 3.], [3., 4.],[4., 5.]], dtype=float32)   
- 현아) broadcasting
array([[2., 3.],
       [3., 4.],
       [4., 5.]], dtype=float32)>>>>>>> master
- 아영)  Broadcasting, [[2.,3.], [3.,4.], [4.,5.]]
- 승렬) broadcasting.
array([[2.,3.]
       [3.,4.]
       [4.,5.]], dtype=float32)
- 지원)  broadcasting / Array([[2.,3.],[3.,4.],[4.,5.]].dtype=float32) 
