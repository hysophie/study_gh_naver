## 현아

### 문제1
Vanishing gradient 문제에 대해 설명하고, 이를 해결하는 2가지 방법을 설명하세요. (RBM 개념을 포함해서 설명하세요.)

### 풀이
- 동건: 1. activation function으로 ReLu를 사용한다. 2. RBM, Wd의 초기값을 결정하는 방법 중 하나로, 두 layer상의 encode와 decode를 반복하여 적절한 초기값을 할당하는 방식. 
- 정우: 
- 현아: 
- 아영:
- 승렬: 
- 지원:

### 문제2
Deep learning에서 layer 개수를 늘릴수록 발생하는 문제와, 이를 해결하는 방법에 대해 설명하세요.

### 풀이
- 동건: vanishing gradients 문제가 발생하고, 이를 해결하기 위해 활성함수를 sigmoid 대신 다른 형태를 사용한다. 
- 정우: 
- 현아:
- 아영:
- 승렬:
- 지원:
---

## 동건

### 문제1

sigmoid와 ReLU 두 함수 값의 차이를 설명하고, 그 차이로 인한 효과를 설명해주세요. 

### 풀이

- 동건: sigmoid는 0~1사이의 값을 갖고, ReLU는 0~무한대의 값을 갖는다. 이 차이로 인해 sigmoid에서 발생하는 vanishing graident 문제가 ReLU에서는 방지된다. 
- 정우: 
- 현아: 
- 아영:
- 승렬: 
- 지원:

## 승렬

### 문제1.
Gradient descent optimizer가 아닌 다른 optimizer들을 각자 하나씩 조사해보고 간략하게 풀이란에 소개해주자! 풀이란의 순서인 데미안~지원 순으로 ML lab 10 영상 11:50에 있는 Adadelta optimizer부터 Ftrl optimizer까지 하나씩 맡아봅시다!

### 풀이
- 동건: first order information 만을 이용하여 동적으로 하강하는 최적화 방법, 학습률을 튜닝할 필요가 없고, 하이퍼 파라미터나 데이터 형식에 강건한 측면이 있다. 대규모 음성 데이터에서 좋은 성능이 확인되었다. 
- 정우: 
- 현아: 
- 아영:
- 승렬: 
- 지원:

## 정우

### 문제1.
1. dropout이 어떤 방식으로 작동하는 지 설명하시오 2. dropout을 구현할 때 주의할 점을 

### 풀이
- 동건: 1. layer상의 임의의 node를 일정 비율 사용하지 않고 학습을 진행한다. 2. 테스트를 할 때는 dropout을 하면 안된다.   
- 정우: 
- 현아: 
- 아영:
- 승렬: 
- 지원:
