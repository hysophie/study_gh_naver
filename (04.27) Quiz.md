## 현아

### 문제1
Vanishing gradient 문제에 대해 설명하고, 이를 해결하는 2가지 방법을 설명하세요. (RBM 개념을 포함해서 설명하세요.)

### 풀이
- 동건: 
- 정우: layer가 많을때 앞에서는 유의미한 기울기가 나타나지만 뒤로 가면 갈수록 기울기가 0에 가까워짐. 그렇기 때문에 맨 뒤에 있는 기울기는 결과에 영향을 미치지 못하게 된다. 결국 학습이 진행되지 않음. 1번째 해결 방법은 sigmoid 대신 relu를 사용한다. 2번째 해결 방법은 weight를 제대로 설정하는 것 여기서는 rbm을 사용하는데 이는 입력값(x)에서 나온 출력값, 출력값에사 입력값을 예측했을때의 예측된 입력값(x2) 이 x2와 x의 차이가 최소인 w를 찾는 방식이다.    
- 현아: 
- 아영:
- 승렬: 
- 지원:

### 문제2
Deep learning에서 layer 개수를 늘릴수록 발생하는 문제와, 이를 해결하는 방법에 대해 설명하세요.

### 풀이
- 동건:
- 정우: 
- 현아:
- 아영:
- 승렬:
- 지원:
---

## 동건

### 문제1

sigmoid와 ReLU 두 함수 식의 차이를 설명하고, 그 차이로 인한 효과를 설명해주세요. 

### 풀이
- 동건: 
- 정우: sigmoid는 범위가 0에서 1사이의 값만 나왔는데 relu는 0보다 작으면 프로그램을 끄고 0보다 크면 계속 커지게 하는것, layer가 많아져도 굉장히 잘 작동하게 되었다. 
- 현아: 
- 아영:
- 승렬: 
- 지원:

## 승렬

### 문제1.
Gradient descent optimizer가 아닌 다른 optimizer들을 각자 하나씩 조사해보고 간략하게 풀이란에 소개해주자! 풀이란의 순서인 데미안~지원 순으로 ML lab 10 영상 11:50에 있는 Adadelta optimizer부터 Ftrl optimizer까지 하나씩 맡아봅시다!

### 풀이
- 동건: 
- 정우: adagrad는 변수들을 udate할 때 각각의 변수의 step siae(learning rate)를 다르게 설정해서 이동하는 방식, 지금까지 많이 변화한 변수의 step size는 작게하고, 많이 변화하지 않는 변수들이 step size는 크게해서 진행하는 방식이다. http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html
- 현아: 
- 아영:
- 승렬: 
- 지원:

## 정우

### 문제1.
1. dropout이 어떤 방식으로 작동하는 지 설명하시오 2. dropout을 구현할 때 주의할 점을 

### 풀이
- 동건: 
- 정우: 1. 몇개의 노드를 랜덤하게 쉬게하고 나머지로만 training을 진행하는 방식, 마지막에 모든 노드를 동원해서 test함. 2. test에 들어가면 dropout rate를 1로 설정하고 해야한다.
- 현아: 
- 아영:
- 승렬: 
- 지원:
